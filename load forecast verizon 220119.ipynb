{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 14:08:41.376460: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-19 14:08:41.380548: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-19 14:08:41.380565: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import forecast_tools as ft\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU, LSTM, Dropout\n",
    "from keras.callbacks import  EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import emd\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "pd.set_option('precision', 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dat(site,filename=None,features='all',emd=True,rename=None,start=None,end=None):\n",
    "\n",
    "  if site == 'deere':\n",
    "    df = pd.read_csv(  '/content/drive/MyDrive/Data/deere_load.csv',   \n",
    "                          comment='#',\n",
    "                          parse_dates=['Datetime (UTC-6)'],\n",
    "                          index_col=['Datetime (UTC-6)'] )\n",
    "\n",
    "  elif site == 'deere-supercleaned':\n",
    "    df = pd.read_csv(  '/content/drive/MyDrive/Data/deere_load_supercleaned.csv',   \n",
    "                          comment='#',\n",
    "                          parse_dates=['Datetime (UTC-6)'],\n",
    "                          index_col=['Datetime (UTC-6)'] )\n",
    "\n",
    "  elif site == 'hyatt':\n",
    "    df = pd.read_csv(  '/content/drive/MyDrive/Data/hyatt_load_IMFs.csv',   \n",
    "                          comment='#',\n",
    "                          parse_dates=['Datetime (UTC-10)'],\n",
    "                          index_col=['Datetime (UTC-10)'] )       \n",
    "    \n",
    "  elif site == 'lajolla':\n",
    "    df = pd.read_csv(  '/content/drive/MyDrive/Data/lajolla_load_IMFs.csv',   \n",
    "                          comment='#',\n",
    "                          parse_dates=['Datetime (UTC-8)'],\n",
    "                          index_col=['Datetime (UTC-8)'] )  \n",
    "\n",
    "  # elif site == 'nwe':\n",
    "  #   df = pd.read_csv(   '/content/drive/MyDrive/Data/NWE/ca_actual.csv',   \n",
    "  #                       comment='#',                 \n",
    "  #                       parse_dates=['Date'],\n",
    "  #                       index_col=['Date'])\n",
    "  #   df = convert_nwe_data_to_vector(df)\n",
    "\n",
    "  elif site == 'terna':\n",
    "    file_path = '/content/drive/MyDrive/Data/terna_load_kw.csv'\n",
    "    dfm = pd.read_csv(  file_path,\n",
    "                        comment='#',\n",
    "                        index_col=0)\n",
    "\n",
    "    idx = pd.date_range(  start   = '2006-1-1 0:00',\n",
    "                          end     = '2015-12-31 23:00',\n",
    "                          freq    = 'H')\n",
    "\n",
    "    df = pd.DataFrame(index=idx, data=np.empty(len(idx)), columns=0)\n",
    "\n",
    "    begin, end = 0, 24\n",
    "    for i in range(dfm.shape[0]):\n",
    "      dat = dfm.iloc[i].values\n",
    "      df.iloc[begin:end,0] = dat\n",
    "      begin, end = begin+24, end+24\n",
    "\n",
    "    # 2006-03-26 02:00:00   NaN\n",
    "    # 2007-03-25 02:00:00   NaN\n",
    "    # 2008-03-30 02:00:00   NaN\n",
    "    # 2009-03-29 02:00:00   NaN\n",
    "    # 2010-03-28 02:00:00   NaN\n",
    "    # 2011-03-27 02:00:00   NaN\n",
    "    # 2012-03-25 02:00:00   NaN\n",
    "    # 2013-03-31 02:00:00   NaN\n",
    "    # 2014-03-30 02:00:00   NaN\n",
    "    # 2015-03-29 02:00:00   NaN\n",
    "    df = df.fillna(method='ffill')\n",
    "    df.iloc[:,0][df.iloc[:,0].isna()]\n",
    "    \n",
    "  else:\n",
    "    df = pd.read_csv(filename,\n",
    "                     comment='#',\n",
    "                     index_col=0,\n",
    "                     parse_dates=True)\n",
    "    \n",
    "  if rename:\n",
    "    df = df.rename(columns={df.columns[0]:rename})\n",
    "\n",
    "  if df.columns[0] != 'Load (kW)':\n",
    "    input('/// Warning pass rename=True to rename (enter to ack): ')\n",
    "    #df = df.rename(columns={df.columns[0]:'Load (kW)'})\n",
    "    \n",
    "  if emd:\n",
    "    df = emd_sift(df)\n",
    "    \n",
    "  if start and end:\n",
    "    df = df.loc[start:end,:]\n",
    "                                    \n",
    "  # df['Day'] = df.index.dayofyear\n",
    "  # df['Hour'] = df.index.hour\n",
    "  # df['Weekday'] = df.index.dayofweek\n",
    "  \n",
    "  df['Day'] = np.abs(df.index.dayofyear - 182)\n",
    "  df['Hour'] = np.abs(np.abs(df.index.hour-12)-12)\n",
    "  df['Weekday'] = np.abs(df.index.dayofweek - 3)\n",
    "  \n",
    "  dppd = {'H':24,'15T':96,'T':1440}[df.index.inferred_freq]\n",
    "    \n",
    "  d = df.iloc[:,0].values.flatten()\n",
    "  rmse_np1d = rmse(d[(dppd*1):],d[:-(dppd*1)])\n",
    "  rmse_np7d = rmse(d[(dppd*7):],d[:-(dppd*7)])\n",
    "\n",
    "  if rmse_np1d < rmse_np7d:\n",
    "    np_days = 1\n",
    "  else:\n",
    "    np_days = 7\n",
    "  \n",
    "  if features=='all':\n",
    "    return df, dppd, np_days\n",
    "  else:\n",
    "    return df[features], dppd, np_days\n",
    "  \n",
    "def one_hot_of_peaks(ds,freq='D'):\n",
    "    df = pd.DataFrame(ds)\n",
    "    df['peak'] = 0\n",
    "    df.loc[df.groupby(pd.Grouper(freq=freq)).idxmax().iloc[:,0], 'peak'] = 1  \n",
    "    return df['peak']     \n",
    "\n",
    "def accuracy_one_hot(true,pred):\n",
    "    \"\"\" Measure the accuracy of two one hot vectors, inputs can be 1d numpy or dataseries\"\"\"\n",
    "    n_misses = sum(true != pred)/2     # every miss gives two 'False' entries\n",
    "    return 1 - n_misses/sum(true)   # basis is the number of one-hots\n",
    "\n",
    "\n",
    "def batch_generator(batch_size, sequence_length, num_x_signals, num_y_signals,\n",
    "                    num_train, x_train_scaled, y_train_scaled):\n",
    "    \"\"\"\n",
    "    Generator function for creating random batches of training-data.\n",
    "    \"\"\"    \n",
    "    # Infinite loop.\n",
    "    while True:\n",
    "        # Allocate a new array for the batch of input-signals.\n",
    "        x_shape = (batch_size, sequence_length, num_x_signals)\n",
    "        x_batch = np.zeros(shape=x_shape, dtype=np.float16)\n",
    "\n",
    "        # Allocate a new array for the batch of output-signals.\n",
    "        y_shape = (batch_size, sequence_length, num_y_signals)\n",
    "        y_batch = np.zeros(shape=y_shape, dtype=np.float16)\n",
    "\n",
    "        # Fill the batch with random sequences of data.\n",
    "        for i in range(batch_size):\n",
    "            # Get a random start-index.\n",
    "            # This points somewhere into the training-data.\n",
    "            idx = np.random.randint(num_train - sequence_length)\n",
    "            \n",
    "            # Copy the sequences of data starting at this index.\n",
    "            x_batch[i] = x_train_scaled[idx:idx+sequence_length]\n",
    "            y_batch[i] = y_train_scaled[idx:idx+sequence_length]\n",
    "        \n",
    "        yield (x_batch, y_batch)     \n",
    "        \n",
    "def rmse(y_true, y_pred):\n",
    "  return np.sqrt(np.mean(np.square(y_true - y_pred)))\n",
    "\n",
    "def emd_sift(df):\n",
    "  \"\"\" Sifts the left-mose column of dataframe\"\"\"\n",
    "  imf = emd.sift.sift(df.iloc[:,0].values)\n",
    "\n",
    "  for i in range(imf.shape[1]):\n",
    "      df['IMF%s'%(i+1)] = imf[:,i]  \n",
    "\n",
    "  return df           "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_4 (GRU)                 (None, None, 24)          2952      \n",
      "                                                                 \n",
      " gru_5 (GRU)                 (None, None, 24)          3600      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, None, 1)           25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,577\n",
      "Trainable params: 6,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "100/100 [==============================] - 7s 54ms/step - loss: 0.2052 - val_loss: 0.3250\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.3250\n",
      "\n",
      "/// Best Model Loss (valid)\n",
      " 0.3249579071998596\n",
      "1/1 [==============================] - 1s 506ms/step\n",
      "\n",
      "/// MAE\n",
      " 3.271182689475915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjw/miniconda3/envs/tf-scip2/lib/python3.8/site-packages/pandas/core/indexing.py:1596: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/home/mjw/miniconda3/envs/tf-scip2/lib/python3.8/site-packages/pandas/core/indexing.py:1734: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for units in [24,48,72,96,128]:\n",
    "#     for sequence_length in [24,48,72,168]:\n",
    "#         for layers in [1,2,3]:\n",
    "#             for afuncs in [{'gru':'relu','dense':'relu'},{'gru':'softmax','dense':'relu'}]:\n",
    "            \n",
    "# data\n",
    "site = 'verizon'\n",
    "filename = '~/data/verizon_west_excelsior_load_solarTMY.csv'\n",
    "data_range = ['2021-9-1','2022-10-11']\n",
    "dir = 'models'\n",
    "features = ['Load (kW)',\n",
    "            'Day',\n",
    "            'Weekday',\n",
    "            'Hour',\n",
    "            'IMF1',                                \n",
    "            'IMF2',                                \n",
    "            'IMF3',\n",
    "            'IMF4',\n",
    "            'IMF5',\n",
    "            'IMF6',\n",
    "            'IMF7',\n",
    "            'IMF8',\n",
    "            'IMF9',\n",
    "            'IMF10',\n",
    "            'IMF11',]\n",
    "targets =  ['LoadTarget'] # LoadTarget | LoadTargetOH\n",
    "\n",
    "# model\n",
    "rnn_type='gru'\n",
    "units=24\n",
    "layers=2\n",
    "dropout=0\n",
    "afuncs={'lstm':'relu','gru':'relu','dense':'relu'}\n",
    "loss='mse'\n",
    "forecast = 'normal' # normal | peak\n",
    "\n",
    "# training\n",
    "sequence_length=96\n",
    "epochs=1\n",
    "patience=20\n",
    "train_split = 0.9\n",
    "shift_steps = 1\n",
    "batch_size=32\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# output\n",
    "verbose=0\n",
    "output = True\n",
    "plots = False\n",
    "#metrics = {'acc':[]}\n",
    "metrics = {'mae':[],'mae_np':[],'skill':[]}\n",
    "\n",
    "# model filename\n",
    "t = datetime.now()\n",
    "path_checkpoint = f'{dir}/{site}/{t.year}-{t.month:02}-{t.day:02}_' + \\\n",
    "                f'{t.hour:02}-{t.minute:02}-{t.second:02}_{rnn_type}-u{units}-l{layers}.keras'\n",
    "\n",
    "\n",
    "### begin\n",
    "\n",
    "df,dppd,np_days = get_dat(site,\n",
    "                          filename,\n",
    "                          emd=True,\n",
    "                          rename='Load (kW)',\n",
    "                          start=data_range[0],\n",
    "                          end=data_range[1])                    \n",
    "\n",
    "if forecast == 'peak':\n",
    "    df['LoadOH'] =      one_hot_of_peaks(df[features[0]])\n",
    "    df['LoadTargetOH'] =   one_hot_of_peaks(df[features[0]]).shift(-shift_steps)\n",
    "    if 'NPOH' in features:\n",
    "        df['NPOH'] =      one_hot_of_peaks(df[features[0]]).shift(np_days*dppd)\n",
    "else:\n",
    "    df['LoadTarget'] = df[features[0]].shift(-shift_steps)\n",
    "\n",
    "if 'NP' in features:\n",
    "    df['NP']         = df[features[0]].shift(np_days*dppd)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# split and shape data\n",
    "num_data = len(df)\n",
    "num_train = int(train_split * num_data)\n",
    "df_train = df.iloc[:num_train,:]\n",
    "df_valid = df.iloc[num_train:,:]\n",
    "X_scaler = MinMaxScaler()\n",
    "X_train = X_scaler.fit_transform(df_train[features].values)\n",
    "X_valid = X_scaler.fit_transform(df_valid[features].values)\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train = y_scaler.fit_transform(df_train[targets].values)\n",
    "y_valid = y_scaler.fit_transform(df_valid[targets].values)\n",
    "generator =    batch_generator( batch_size,\n",
    "                                sequence_length,\n",
    "                                num_x_signals=len(features),\n",
    "                                num_y_signals=len(targets),\n",
    "                                num_train=num_train,\n",
    "                                x_train_scaled=X_train,\n",
    "                                y_train_scaled=y_train)   \n",
    "X_valid = X_valid[np.newaxis,:,:]\n",
    "y_valid = y_valid[np.newaxis,:,:] \n",
    "\n",
    "# build and train model\n",
    "# convention: X and y are scaled values, 'Load' etc are not\n",
    "if rnn_type == 'gru':\n",
    "    model = Sequential()\n",
    "    model.add( GRU( units=units,\n",
    "                    return_sequences=True,\n",
    "                    input_shape=(None, len(features),),\n",
    "                    activation=afuncs[rnn_type]) )\n",
    "    if layers > 1:\n",
    "        model.add( GRU( units=units,\n",
    "                        return_sequences=True,\n",
    "                        activation=afuncs[rnn_type]) )\n",
    "    if layers > 2:\n",
    "        model.add( GRU( units=units,\n",
    "                        return_sequences=True,\n",
    "                        activation=afuncs[rnn_type]) )        \n",
    "    model.add( Dense(units=len(targets),\n",
    "                        activation=afuncs['dense']) )\n",
    "elif rnn_type == 'lstm':\n",
    "    if layers==1:\n",
    "        model = Sequential([LSTM(units=units,\n",
    "                                    return_sequences=True,\n",
    "                                    input_shape=(None,len(features)),\n",
    "                                    activation=afuncs[rnn_type]),\n",
    "                            Dense(units=len(targets),\n",
    "                                    activation=afuncs[rnn_type]) ] )\n",
    "    elif layers==2:\n",
    "        model = Sequential([LSTM(units=units,\n",
    "                                    return_sequences=True,\n",
    "                                    input_shape=(None,len(features)),\n",
    "                                    activation=afuncs[rnn_type]),\n",
    "                            LSTM(units=units,\n",
    "                                    return_sequences=True,\n",
    "                                    activation=afuncs[rnn_type]), \n",
    "                            Dense(units=len(targets),\n",
    "                                    activation=afuncs[rnn_type]) ] )\n",
    "model.compile( loss=loss, optimizer='adam')\n",
    "model.summary()                \n",
    "hx = model.fit( x=generator,\n",
    "                epochs=epochs,\n",
    "                steps_per_epoch=100,\n",
    "                validation_data=(X_valid,y_valid),\n",
    "                callbacks=[ ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                            monitor='val_loss',\n",
    "                                            verbose=verbose,\n",
    "                                            save_weights_only=True,\n",
    "                                            save_best_only=True         ),\n",
    "                            EarlyStopping  (monitor='val_loss',\n",
    "                                            patience=patience,\n",
    "                                            verbose=verbose             )   ])        \n",
    "\n",
    "# eval\n",
    "model.load_weights(path_checkpoint) # just in case, load best model\n",
    "print('\\n/// Best Model Loss (valid)\\n',model.evaluate(X_valid,y_valid))\n",
    "\n",
    "# already have this in 'LoadTargets'\n",
    "#df_valid.loc[:,'Targets'] =       y_scaler.inverse_transform(y_valid.flatten()[:,np.newaxis]                  )\n",
    "\n",
    "df_valid.loc[:,'Predictions'] =  y_scaler.inverse_transform(model.predict(X_valid).flatten()[:,np.newaxis]   )\n",
    "\n",
    "if forecast == 'peak':\n",
    "    acc = accuracy_one_hot(df_valid.LoadTargetOH,one_hot_of_peaks(df_valid.Predictions))\n",
    "    print('\\n/// Accuracy\\n',acc)\n",
    "    metrics['acc'].append(acc)\n",
    "else:\n",
    "    mae_np = df_valid.LoadTarget.diff(np_days*dppd).dropna().abs().mean()\n",
    "    mae = (df_valid.LoadTarget - df_valid.Predictions).abs().mean()\n",
    "    skill = 1 - mae/mae_np\n",
    "    metrics['mae'].append(mae)\n",
    "    metrics['mae_np'].append(mae_np)\n",
    "    metrics['skill'].append(skill)\n",
    "    print('\\n/// MAE\\n',mae)\n",
    "    \n",
    "# save \n",
    "pd.DataFrame(metrics).to_csv('results.csv')\n",
    "\n",
    "# plot\n",
    "#pd.DataFrame(hx.history).plot(title=f'Accuracy = {acc:.3f}')  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'verizon'\n",
    "filename = '~/data/verizon_west_excelsior_load_solarTMY.csv'\n",
    "data_range = ['2021-9-1','2022-10-11']\n",
    "df,dppd,np_days = get_dat(site,\n",
    "                          filename,\n",
    "                          emd=True,\n",
    "                          rename='Load (kW)',\n",
    "                          start=data_range[0],\n",
    "                          end=data_range[1])                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-scip2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f8d946f6c6df69c3c75f6ca5805fbe3bcdfda4a846ef7d62a7da9bc134420d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
